{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce8a754",
   "metadata": {},
   "source": [
    "# Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbd16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from enum import Enum\n",
    "from collections import deque\n",
    "from random import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from operator import add\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a8d87",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Environment exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da0f0a9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Cartpole-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4fa69d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### sample components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34627a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cartpole_env = gym.make('CartPole-v1')\n",
    "state = cartpole_env.reset()\n",
    "print(\"\\n***action***\")\n",
    "print(cartpole_env.action_space)\n",
    "print(\"\\n***State***\")\n",
    "print(state, cartpole_env.state.shape)\n",
    "print(\"observationspace \", cartpole_env.observation_space.shape[0])\n",
    "print(\"actionspace,\" ,cartpole_env.action_space.n)\n",
    "\n",
    "\n",
    "state, reward, done, _ = cartpole_env.step(cartpole_env.action_space.sample())\n",
    "\n",
    "print(\"\\n*** reward ***\")\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf984ab2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### visualizing env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd947b4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cartpole_env.reset()\n",
    "for _ in range(1000):\n",
    "    cartpole_env.render()\n",
    "    _, _, done, _ = cartpole_env.step(cartpole_env.action_space.sample())\n",
    "    if done:\n",
    "        break\n",
    "cartpole_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e5365",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## LunarLander-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52114314",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### sample components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d558ae0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lun_env = gym.make('LunarLander-v2')\n",
    "print(\"\\n*** action ***\")\n",
    "print(lun_env.action_space)\n",
    "\n",
    "state = lun_env.reset()\n",
    "print(\"\\n*** state ***\")\n",
    "print(state, state.shape)\n",
    "\n",
    "s, r, done, _ = lun_env.step(lun_env.action_space.sample())\n",
    "\n",
    "print(\"\\n*** reward ***\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1102c1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### visualizing env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b6c09",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lun_env.reset()\n",
    "for _ in range(1000):\n",
    "    lun_env.render()\n",
    "    _, _, done, _ = lun_env.step(lun_env.action_space.sample())\n",
    "    if done:\n",
    "        break\n",
    "lun_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e75844",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b085e",
   "metadata": {},
   "source": [
    "## Define q-value architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1aa53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FCNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, device):\n",
    "        super(FCNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 300)\n",
    "        self.fc2 = nn.Linear(300, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e6ca9a",
   "metadata": {},
   "source": [
    "## Define agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493db4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DqnVannila:\n",
    "    def __init__(self, sync_steps, buffer_size, gamma, lr, eps, state_size, action_size, batch_size, device):\n",
    "        self.sync_steps = sync_steps\n",
    "        self.max_buffer_size = buffer_size\n",
    "        self.gamma = torch.tensor(gamma)\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.buffer = deque()\n",
    "        self.action_size = action_size\n",
    "        self.q = FCNet(state_size, action_size, device=device)\n",
    "        self.q_target = FCNet(state_size, action_size, device=device)\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.optimizer = optim.Adam(self.q.parameters(), lr=lr)\n",
    "        self.update_step = 0\n",
    "        \n",
    "    def get_q_val(self, qnet, state, action=None):\n",
    "        if action is None:\n",
    "            return qnet(state)\n",
    "        return qnet(state).gather(1, action.reshape(-1,1).long())\n",
    "        \n",
    "    def add_to_buffer(self, experience):\n",
    "        if len(self.buffer) >= self.max_buffer_size:\n",
    "            self.buffer.pop()\n",
    "        self.buffer.appendleft(experience)\n",
    "        \n",
    "    def _np_to_tensor(self, nparrs):\n",
    "        return [torch.from_numpy(arr).float().to(self.device) for arr in nparrs]\n",
    "        \n",
    "    def sample_minibatch(self):\n",
    "        idxs = np.random.randint(0, len(self.buffer), self.batch_size)\n",
    "        sample = [self.buffer[i] for i in idxs]\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = list(map(lambda x: np.array(x, dtype=np.float64), zip(*sample)))\n",
    "        return tuple(self._np_to_tensor([states, actions, rewards, next_states, dones]))\n",
    "    \n",
    "    def synchronize(self):\n",
    "        self.q_target.load_state_dict(self.q.state_dict())\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        if random() < self.eps:\n",
    "            return np.random.randint(0, self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return np.argmax(self.get_q_val(self.q, state).numpy())\n",
    "        \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        self.update_step += 1\n",
    "        \n",
    "        #TODO:implement epsilon decay properly\n",
    "        new_eps = self.eps - (self.update_step/1000)*self.eps\n",
    "        self.eps = max(0, new_eps)\n",
    "        \n",
    "        self.add_to_buffer([state, action, reward, next_state, done])\n",
    "        \n",
    "        #TODO: warmup. should be configurable.\n",
    "        if len(self.buffer) < 1000:\n",
    "            return\n",
    "        \n",
    "        batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = self.sample_minibatch()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            last_step_target = batch_dones * batch_rewards\n",
    "            middle_step_target = (1-batch_dones)*(batch_rewards + self.gamma*torch.max(self.get_q_val(self.q_target, batch_next_states), dim=1).values)\n",
    "            targets =  last_step_target + middle_step_target\n",
    "        \n",
    "        predictions = self.get_q_val(self.q, batch_states, batch_actions).squeeze()\n",
    "        loss = F.mse_loss(predictions, targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_step % self.sync_steps == 0:\n",
    "            self.synchronize()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e582d65",
   "metadata": {},
   "source": [
    "## nasty training for test (should be removed later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "done = False\n",
    "state = env.reset()\n",
    "\n",
    "dqn = DqnVannila(\n",
    "    sync_steps=100,\n",
    "    buffer_size=1000,\n",
    "    gamma=0.99,\n",
    "    lr=0.001,\n",
    "    eps=0.6,\n",
    "    state_size=env.observation_space.shape[0],\n",
    "    action_size=env.action_space.n,\n",
    "    batch_size=128,\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    ")\n",
    "\n",
    "#TODO: implement a trainer function which does the following:\n",
    "#      takes epochs, env and solver -> trains and returns required graphs and metrics\n",
    "returns = []\n",
    "for _ in tqdm(range(1000)):\n",
    "    ret = 0\n",
    "    while not done:\n",
    "        a = dqn.get_action(state)\n",
    "        next_state, r, done, _ = env.step(a)\n",
    "        dqn.update(state, a, r, next_state, done)\n",
    "        ret += r\n",
    "        state = next_state\n",
    "    done = False\n",
    "    env.reset()\n",
    "    returns.append(ret)\n",
    "    \n",
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015caab7",
   "metadata": {},
   "source": [
    "# GridWorld environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd74078",
   "metadata": {},
   "source": [
    "## Defining environment objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0845f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldObj:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \" \"\n",
    "    \n",
    "class Empty(WorldObj):\n",
    "    def __init__(self):\n",
    "        super().__init__('empty')\n",
    "    \n",
    "class Log(WorldObj):\n",
    "    def __init__(self):\n",
    "        super().__init__('log')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '_'\n",
    "    \n",
    "class Gold(WorldObj):\n",
    "    def __init__(self):\n",
    "        super().__init__('gold')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'G'\n",
    "    \n",
    "class Food(WorldObj):\n",
    "    def __init__(self):\n",
    "        super().__init__('food')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '8'\n",
    "    \n",
    "    \n",
    "class Trap(WorldObj):\n",
    "    def __init__(self):\n",
    "        super().__init__('trap')\n",
    "        self.state = None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'X'\n",
    "\n",
    "class Teleport(WorldObj):\n",
    "    def __init__(self, dest, chance=0.8):\n",
    "        super().__init__('teleport')\n",
    "        self.destination = dest\n",
    "        self.chance = chance\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'T'\n",
    "    \n",
    "    \n",
    "class Stream(WorldObj):\n",
    "    \"\"\"\n",
    "    stream only goes down or right!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, _dir: int):\n",
    "        super().__init__('stream')\n",
    "        self.dir = _dir\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '~'\n",
    "    \n",
    "class Miner(WorldObj):\n",
    "    def __init__(self):\n",
    "        super().__init__('miner')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '*'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148cac3e",
   "metadata": {},
   "source": [
    "## Defining environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    RIGHT = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    UP = 3\n",
    "    LOG_RIGHT = 4\n",
    "    LOG_DOWN = 5\n",
    "    LOG_LEFT = 6\n",
    "    LOG_UP = 7\n",
    "    \n",
    "    \n",
    "\n",
    "class GoldHuntEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    static_map_size = 7\n",
    "    movement_acts = [Action.RIGHT, Action.LEFT, Action.DOWN, Action.UP,]\n",
    "    craft_acts = [Action.LOG_RIGHT, Action.LOG_LEFT, Action.LOG_DOWN, Action.LOG_UP,]\n",
    "\n",
    "    def __init__(self, ascii_rep=True, food_prob=0.6, teleport_prob=0.8):\n",
    "        self.ascii_rep = ascii_rep\n",
    "        self.action_space = gym.spaces.Discrete(8)\n",
    "        self.observation_space = gym.spaces.Box(-1, 1, shape=(2,))\n",
    "        self.info = {}\n",
    "        self.static_map_size = 7\n",
    "        self.max_time_step = self.static_map_size * 4\n",
    "        self.max_reward_size = 10\n",
    "        self.food_reward = 5 / self.max_reward_size\n",
    "        self.gold_reward = 10 / self.max_reward_size\n",
    "        self.time_punishment = -1 / self.max_reward_size\n",
    "        self.trap_falling_reward = -5 / self.max_reward_size\n",
    "        self.craft_log_reward = -3 / self.max_reward_size\n",
    "        self.gold_loc = [6, 6]\n",
    "        self.food_prob = food_prob\n",
    "        self.teleport_prob = teleport_prob\n",
    "        print(\"observation_Space\", self.observation_space.shape)\n",
    "        print(\"action_space \", self.action_space.n)\n",
    "        \n",
    "    def generate_static_map(self, agent_pos):\n",
    "        env_map = [[Empty() for _ in range(self.map_size)] for _ in range(self.map_size)]\n",
    "        \n",
    "        env_map[2][0] = Trap()\n",
    "        env_map[2][1] = Trap()\n",
    "        env_map[2][2] = Trap()\n",
    "        env_map[2][3] = Trap()\n",
    "        env_map[1][3] = Trap()\n",
    "        env_map[0][3] = Trap()\n",
    "        env_map[5][3] = Trap()\n",
    "        env_map[6][3] = Trap()\n",
    "        \n",
    "        env_map[self.gold_loc[0]][self.gold_loc[1]] = Gold()\n",
    "        \n",
    "        if self.teleport_prob > 0:\n",
    "            env_map[0][6] = Teleport(dest=[6,0])\n",
    "            \n",
    "        env_map[4][0] = Stream(_dir=Action.LEFT.value)\n",
    "        env_map[4][1] = Stream(_dir=Action.LEFT.value)\n",
    "        env_map[4][2] = Stream(_dir=Action.LEFT.value)\n",
    "        env_map[4][3] = Stream(_dir=Action.LEFT.value)\n",
    "        env_map[4][4] = Stream(_dir=Action.LEFT.value)\n",
    "        env_map[4][5] = Stream(_dir=Action.LEFT.value)\n",
    "        env_map[4][6] = Stream(_dir=Action.LEFT.value)\n",
    "        \n",
    "            \n",
    "        return env_map\n",
    "\n",
    "    def get_pos_obj(self, pos):\n",
    "        return self.env_map[pos[0]][pos[1]]\n",
    "\n",
    "    def stochastic_generation(self, pos, obj, prob):\n",
    "        if random() < prob:\n",
    "            self.env_map[pos[0]][pos[1]] = obj\n",
    "    \n",
    "    def _get_next_pos(self, current_loc, action):\n",
    "        action_dir = Action(action)\n",
    "        next_pos = copy(current_loc)\n",
    "        \n",
    "        if action_dir not in self.movement_acts:\n",
    "            return current_loc\n",
    "        \n",
    "        if action_dir == Action.RIGHT:\n",
    "            next_pos[1] = self._restricted_move(next_pos[1], 1)\n",
    "        elif action_dir == Action.LEFT:\n",
    "            next_pos[1] = self._restricted_move(next_pos[1], -1)\n",
    "        elif action_dir == Action.UP:\n",
    "            next_pos[0] = self._restricted_move(next_pos[0], -1)\n",
    "        elif action_dir == Action.DOWN:\n",
    "            next_pos[0] = self._restricted_move(next_pos[0], 1)\n",
    "            \n",
    "        return next_pos\n",
    "        \n",
    "        \n",
    "    def _restricted_move(self, init_val, res):\n",
    "        new_val = init_val + res\n",
    "        if new_val >= self.map_size:\n",
    "            new_val = self.map_size - 1\n",
    "        elif new_val < 0:\n",
    "            new_val = 0\n",
    "        return new_val\n",
    "    \n",
    "    \n",
    "    def pos_to_state(self, pos):\n",
    "        mapped_pos = [0, 0]\n",
    "        mapped_pos[0] = pos[0] / (self.map_size/2) - 1\n",
    "        mapped_pos[1] = pos[1] / (self.map_size/2) - 1\n",
    "        return np.array(mapped_pos)\n",
    "    \n",
    "    def on_border(self, pos):\n",
    "        if pos[0] == 0 or pos[0] == self.map_size-1 or pos[1] == 0 or pos[1] == self.map_size-1:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def craft_log(self, current_pos, action: int):\n",
    "        \"\"\"\n",
    "        returns True if log is crafted, False otherwise\n",
    "        \"\"\"\n",
    "        log_loc = self._get_next_pos(current_pos, action-4)\n",
    "        if log_loc == current_pos or log_loc == self.gold_loc:\n",
    "            return False\n",
    "        \n",
    "        self.agent_pos = log_loc\n",
    "        self.env_map[log_loc[0]][log_loc[1]] = Log()\n",
    "        return True\n",
    "    \n",
    "    def generate_stochastic_elements(self):\n",
    "        self.stochastic_generation([1,6], Food(), self.food_prob)\n",
    "        self.stochastic_generation([0,5], Food(), self.food_prob)\n",
    "        self.stochastic_generation([1,5], Food(), self.food_prob)\n",
    "        self.stochastic_generation([5,1], Food(), self.food_prob)\n",
    "        self.stochastic_generation([5,0], Food(), self.food_prob)\n",
    "        self.stochastic_generation([6,1], Food(), self.food_prob)\n",
    "    \n",
    "    def step(self, action: int):\n",
    "        self.info['statu'] = \"\"\n",
    "        self.info['step'] = self.time_step\n",
    "        self.time_step += 1\n",
    "        \n",
    "        done = self.time_step > self.max_time_step \n",
    "        reward = self.time_punishment\n",
    "        \n",
    "        self.info['action'] = f\"agent performed {Action(action)}\"\n",
    "        curr_obj = self.get_pos_obj(self.agent_pos)\n",
    "        \n",
    "        # agent was trapped before\n",
    "        if isinstance(curr_obj, Trap) and Action(action) in self.movement_acts:\n",
    "            # this part shows that trap needs memory \n",
    "            #             trap_state = curr_obj.state\n",
    "#             if trap_state is None or action != (trap_state+2)%4:\n",
    "#                 self.info['status'] = f\"Agent in trap with previous action of {None if curr_obj.state is None else Action(curr_obj.state)}\"\n",
    "#                 curr_obj.state = action\n",
    "#                 self.info['cum_r'] += reward\n",
    "#                 return self.pos_to_state(self.agent_pos), reward, done, self.info\n",
    "#             else:\n",
    "#                 self.info['status'] = f\"Agent broke the trap\"\n",
    "#                 self.env_map[self.agent_pos[0]][self.agent_pos[1]] = Empty()\n",
    "            self.env_map[self.agent_pos[0]][self.agent_pos[1]] = Empty()        \n",
    "        \n",
    "        # agent crafts a log\n",
    "        if Action(action) in self.craft_acts:\n",
    "            is_crafted = self.craft_log(self.agent_pos, action)\n",
    "            \n",
    "            if is_crafted:\n",
    "                self.info['status'] = 'agent crafted a log'\n",
    "                reward = self.craft_log_reward\n",
    "            \n",
    "            self.info['cum_r'] += reward\n",
    "            return self.pos_to_state(self.agent_pos), reward, done, self.info\n",
    "        \n",
    "\n",
    "        next_pos = self._get_next_pos(self.agent_pos, action)\n",
    "        next_obj = self.get_pos_obj(next_pos)\n",
    "\n",
    "        # stream takes agent when he falls into it\n",
    "        if isinstance(next_obj, Stream):\n",
    "            self.info['status'] = ('agent fell into a stream')\n",
    "            while isinstance(next_obj, Stream):\n",
    "                next_pos = self._get_next_pos(next_pos, next_obj.dir)\n",
    "                next_obj = self.get_pos_obj(next_pos)\n",
    "                \n",
    "                if isinstance(next_obj, Stream) and self.on_border(next_pos):\n",
    "                    if next_pos[0] == self.map_size-1 and next_pos[1] == self.map_size-1:\n",
    "                        done = True\n",
    "                        reward = -20\n",
    "                    elif next_pos[0] == self.map_size-1 and Action(next_obj.dir) == Action.DOWN:\n",
    "                        next_pos[1] = self._restricted_move(next_pos[1], 1)\n",
    "                    elif next_pos[0] == 0 and Action(next_obj.dir) == Action.UP:\n",
    "                        next_pos[1] = self._restricted_move(next_pos[1], -1)\n",
    "                    elif next_pos[1] == self.map_size-1 and Action(next_obj.dir) == Action.RIGHT:\n",
    "                        next_pos[0] = self._restricted_move(next_pos[0], 1)\n",
    "                    elif next_pos[1] == 0 and Action(next_obj.dir) == Action.LEFT:\n",
    "                        next_pos[0] = self._restricted_move(next_pos[0], -1)\n",
    "                    \n",
    "                next_obj = self.get_pos_obj(next_pos)                              \n",
    "        \n",
    "        # gets hurt by falling into trap\n",
    "        if isinstance(next_obj, Trap):\n",
    "            self.info['status'] = 'agent fell into a trap'\n",
    "            reward = self.trap_falling_reward\n",
    "            \n",
    "        if isinstance(next_obj, Teleport):\n",
    "            \"\"\" manually non deterministic setup for this env \"\"\"\n",
    "            \n",
    "            if random() <= next_obj.chance:\n",
    "                self.info['status'] = f'agent teleported to {next_obj.destination} '\n",
    "                self.agent_pos = next_obj.destination\n",
    "                self._clear_pos(next_obj.destination)\n",
    "                return self.pos_to_state(self.agent_pos), reward, done, self.info\n",
    "        \n",
    "        # refills energy by eating food\n",
    "        if isinstance(next_obj, Food):\n",
    "            self.info['status'] = 'agent ate food!'\n",
    "            self._clear_pos(next_pos)\n",
    "            reward = self.food_reward\n",
    "        \n",
    "        # agent finds the gold\n",
    "        if isinstance(next_obj, Gold):\n",
    "            self.info['status'] = 'agent found the GOLD!'\n",
    "            done = True\n",
    "            reward = self.gold_reward\n",
    "            \n",
    "        self.agent_pos = next_pos\n",
    "        self.info['cum_r'] += reward\n",
    "        \n",
    "        return self.pos_to_state(self.agent_pos), reward, done, self.info\n",
    "\n",
    "    def _clear_pos(self, pos):\n",
    "        self.env_map[pos[0]][pos[1]] = Empty()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.info = {\n",
    "            'cum_r': 0,\n",
    "            'action': \"\",\n",
    "            'status': \"\",\n",
    "            'step': 0\n",
    "        }\n",
    "        \n",
    "        self.map_size = self.static_map_size\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.env_map = self.generate_static_map(self.agent_pos)    \n",
    "        self.generate_stochastic_elements()\n",
    "        self.prev_action = None\n",
    "        self.time_step = 0\n",
    "        \n",
    "        return self.pos_to_state(self.agent_pos)\n",
    "  \n",
    "    def get_env_rep(self):\n",
    "        env_map_top_layer = deepcopy(self.env_map)\n",
    "        env_map_top_layer[self.agent_pos[0]][self.agent_pos[1]] = Miner()\n",
    "        \n",
    "        return tabulate(env_map_top_layer, headers=[], tablefmt='grid')\n",
    "\n",
    "    def get_rgb_rep(self):\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.set_xlim(0, self.map_size)\n",
    "        ax.set_ylim(0, self.map_size)\n",
    "        \n",
    "        show_layer = deepcopy(self.env_map)\n",
    "        show_layer[self.agent_pos[0]][self.agent_pos[1]] = Miner()\n",
    "        \n",
    "        for i in range(len(show_layer)):\n",
    "            for j in range(len(show_layer)):\n",
    "                obj = show_layer[i][j]\n",
    "                \n",
    "                if isinstance(obj, Gold):\n",
    "                    path = './images/gold.png'\n",
    "                elif isinstance(obj, Trap):\n",
    "                    path = './images/trap.png'\n",
    "                elif isinstance(obj, Food):\n",
    "                    path = './images/food.png'\n",
    "                elif isinstance(obj, Log):\n",
    "                    path = './images/log.png'\n",
    "                elif isinstance(obj, Stream):\n",
    "                    path = './images/stream.png'\n",
    "                elif isinstance(obj, Teleport):\n",
    "                    path = './images/teleport.png'\n",
    "                elif isinstance(obj, Miner):\n",
    "                    path = './images/agent.png'\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                \n",
    "                obj_repr = AnnotationBbox(OffsetImage(plt.imread(path), zoom=0.28),\n",
    "                       list(map(add, [i, j], [0.5, 0.5])), frameon=False)\n",
    "                ax.add_artist(obj_repr)\n",
    "        \n",
    "        plt.xticks([i for i in range(self.map_size)])\n",
    "        plt.yticks([i for i in range(self.map_size)])\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if self.ascii_rep:\n",
    "            print(self.get_env_rep())\n",
    "        else:\n",
    "            self.get_rgb_rep()\n",
    "        \n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "env = GoldHuntEnv(ascii_rep=False)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55312f7a",
   "metadata": {},
   "source": [
    "## training dqn on gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eefebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDQN(env, buffer_size, gamma_val, learning_rate, epsilon_val, batch_size):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    \n",
    "    dqn = DqnVannila(\n",
    "        sync_steps=100,\n",
    "        buffer_size=buffer_size,\n",
    "        gamma=gamma_val,\n",
    "        lr=learning_rate,\n",
    "        eps=epsilon_val,\n",
    "        state_size=env.observation_space.shape[0],\n",
    "        action_size=(env.action_space.n),\n",
    "        batch_size=128,\n",
    "        device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    )\n",
    "\n",
    "    returns = []\n",
    "    for _ in tqdm(range(1000)):\n",
    "        ret = 0\n",
    "        while not done:\n",
    "            a = dqn.get_action(state)\n",
    "            next_state, r, done, _ = env.step(a)\n",
    "            dqn.update(state, a, r, next_state, done)\n",
    "            ret += r\n",
    "            state = next_state\n",
    "        done = False\n",
    "        env.reset()\n",
    "        returns.append(ret)\n",
    "\n",
    "    plt.plot(returns)\n",
    "    return dqn\n",
    "    \n",
    "gold_hunt_env = GoldHuntEnv(ascii_rep=False)\n",
    "gold_hunt_env.reset()\n",
    "\n",
    "dqn = trainDQN(\n",
    "    env = gold_hunt_env,\n",
    "    buffer_size=1000,\n",
    "    gamma_val=0.99,\n",
    "    learning_rate=0.01,\n",
    "    epsilon_val=0.06,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d12fe8",
   "metadata": {},
   "source": [
    "## play in environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef049c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def play(env, agent):\n",
    "    done = False\n",
    "    clear_output(wait=True)\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print(info)\n",
    "        env.render()\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        \n",
    "play(gold_hunt_env, dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295fede5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
